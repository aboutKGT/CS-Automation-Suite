import schedule
import time
import asyncio
import os
import random

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤ ê°€ì ¸ì˜¤ê¸°
from src.crawler import GlowmCrawler
from src.processor import ReviewProcessor
from src.notifier import SlackNotifier
from src.storage import ReviewStorage

# --- [ì„¤ì • êµ¬ê°„] ---
CHECK_INTERVAL_MINUTES = 30 
TARGET_URL = "https://theglowm.com/product/%EA%B8%80%EB%A1%9C%EC%9A%B0%EC%97%A0-%ED%94%84%EB%A6%AC%EB%AF%B8%EC%97%84-%ED%95%98%EB%93%9C-%EC%BC%80%EC%9D%B4%EC%8A%A4/46/category/42/display/1/"

def job():
    current_time = time.strftime('%Y-%m-%d %H:%M:%S')
    print(f"\nâ° [ìŠ¤ì¼€ì¤„ëŸ¬] ë¦¬ë·° ê²€í†  ì‹œì‘ ({current_time})...")
    
    crawler = GlowmCrawler()
    processor = ReviewProcessor()
    notifier = SlackNotifier()
    storage = ReviewStorage()

    is_first_run = not os.path.exists("data/reviews_db.csv")
    max_pages = 20 if is_first_run else 100

    print(f"   ğŸ‘‰ ì „ëµ: {'ìµœì´ˆ êµ¬ì¶• ëª¨ë“œ' if is_first_run else 'ëª¨ë‹ˆí„°ë§ ëª¨ë“œ'}")

    try:
        # 1. ì‹ ê·œ ë¦¬ë·° ìˆ˜ì§‘
        new_reviews = asyncio.run(
            crawler.fetch_reviews(TARGET_URL, max_pages=max_pages, storage=storage)
        )
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    if not new_reviews:
        print(f"ğŸ’¤ ìƒˆë¡œ ë“±ë¡ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš€ {len(new_reviews)}ê°œì˜ ì‹ ê·œ ë¦¬ë·° ë°œê²¬! Batch ë¶„ì„ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.")

    # 2. Batch ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° í¬ë§·íŒ…
    # ë¶„ì„ì— í•„ìš”í•œ IDì™€ í…ìŠ¤íŠ¸ ìŒì„ ë§Œë“­ë‹ˆë‹¤.
    batch_input = []
    raw_text_map = {} # ë‚˜ì¤‘ì— ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì°¾ê¸° ìœ„í•œ ë§¤í•‘ìš©
    
    for text in new_reviews:
        r_id = storage.generate_id(text)
        batch_input.append({'id': r_id, 'text': text})
        raw_text_map[r_id] = text

    # 3. AI Batch ë¶„ì„ ì‹¤í–‰ (í•œ ë²ˆì— ë¬¶ì–´ì„œ ì „ì†¡!)
    print(f"ğŸ§  Gemini Batch ë¶„ì„ ì¤‘... (ë¦¬ë·° {len(batch_input)}ê°œ)")
    analysis_results = processor.analyze_reviews_batch(batch_input)

    # 4. ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ë° ì•Œë¦¼
    success_count = 0
    if analysis_results:
        for result in analysis_results:
            r_id = result.get('id')
            if not r_id or r_id not in raw_text_map:
                continue
                
            # ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ê²°ê³¼ í•©ì¹˜ê¸°
            result['raw_text'] = raw_text_map[r_id]
            
            # DB ì €ì¥
            storage.save_review(result)
            
            # ìŠ¬ë™ ì „ì†¡ (ìµœì´ˆ êµ¬ì¶• ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ë°œì†¡í•˜ê±°ë‚˜ í•„í„°ë§ ê°€ëŠ¥)
            notifier.send_notification(result)
            success_count += 1
            
            # ìŠ¬ë™ ë©”ì‹œì§€ ê°„ê²© ìœ ì§€ (ìŠ¬ë™ API ê°€ì´ë“œ ì¤€ìˆ˜)
            time.sleep(1) 
    else:
        print("âš ï¸ Batch ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ ì‘ì—… ì™„ë£Œ! ì´ {success_count}ê±´ì˜ ì‹ ê·œ ë¦¬ë·°ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")

# --- [ì„œë²„ ê°€ë™ ë£¨í”„] ---
schedule.every(CHECK_INTERVAL_MINUTES).minutes.do(job)

print(f"ğŸš€ [GLOW.M] CS ìë™í™” ì„œë²„ ê°€ë™ ì‹œì‘ (Batch ëª¨ë“œ)")
print(f"   - ì£¼ê¸°: {CHECK_INTERVAL_MINUTES}ë¶„")
print(f"   - íƒ€ê²Ÿ: {TARGET_URL}")

# ì¦‰ì‹œ ì‹¤í–‰
job()

while True:
    try:
        schedule.run_pending()
        time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    except Exception as e:
        print(f"\nâŒ ìŠ¤ì¼€ì¤„ëŸ¬ ë£¨í”„ ì—ëŸ¬: {e}")
        time.sleep(60)